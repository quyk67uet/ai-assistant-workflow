import os
import json
from datetime import datetime
from typing import Dict, Any, List
from dotenv import load_dotenv
import google.generativeai as genai
from google.generativeai.types import FunctionDeclaration, Tool

from tools import assign_exercise, get_student_activity_log, grade_submission, add_note_to_report, create_custom_pathway, list_available_submissions

# Load environment variables
load_dotenv()


def configure_gemini():
    """Configure Gemini API with tools."""
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    
    # Define function declarations for Gemini
    assign_exercise_func = FunctionDeclaration(
        name="assign_exercise",
        description="Assign exercises to a student based on learning object title",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student to assign exercises to"
                },
                "learning_object_title": {
                    "type": "string",
                    "description": "The title of the learning object/topic"
                },
                "num_questions": {
                    "type": "integer",
                    "description": "Number of questions to assign"
                }
            },
            "required": ["student_name", "learning_object_title", "num_questions"]
        }
    )
    
    get_activity_log_func = FunctionDeclaration(
        name="get_student_activity_log",
        description="Get activity log for a specific student",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student"
                },
                "date_range": {
                    "type": "string",
                    "description": "Date range filter (e.g., 'today', 'this_week', etc.)"
                }
            },
            "required": ["student_name"]
        }
    )
    
    grade_submission_func = FunctionDeclaration(
        name="grade_submission",
        description="Grade a student submission and provide feedback",
        parameters={
            "type": "object",
            "properties": {
                "submission_id": {
                    "type": "string",
                    "description": "The ID of the submission to grade"
                },
                "score": {
                    "type": "number",
                    "description": "The score to assign (0-100)"
                },
                "feedback_text": {
                    "type": "string",
                    "description": "Detailed feedback text for the student"
                }
            },
            "required": ["submission_id", "score", "feedback_text"]
        }
    )
    
    add_note_func = FunctionDeclaration(
        name="add_note_to_report",
        description="Add a tutor note to student's progress report",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student"
                },
                "note_text": {
                    "type": "string",
                    "description": "The note text to add to the student's report"
                }
            },
            "required": ["student_name", "note_text"]
        }
    )
    
    create_pathway_func = FunctionDeclaration(
        name="create_custom_pathway",
        description="Create a custom learning pathway for a student with specific learning objects",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student"
                },
                "learning_object_titles": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "List of learning object titles to include in the pathway"
                }
            },
            "required": ["student_name", "learning_object_titles"]
        }
    )
    
    list_submissions_func = FunctionDeclaration(
        name="list_available_submissions",
        description="List all available submissions that can be graded",
        parameters={
            "type": "object",
            "properties": {},
            "required": []
        }
    )
    
    # Create tool with function declarations
    tool = Tool(function_declarations=[
        assign_exercise_func, 
        get_activity_log_func, 
        grade_submission_func, 
        add_note_func, 
        create_pathway_func,
        list_submissions_func
    ])
    
    # Initialize model with tools and system instruction
    system_instruction = """
    B·∫°n l√† ISY - tr·ª£ l√Ω AI th√¥ng minh cho gia s∆∞, chuy√™n h·ªó tr·ª£ qu·∫£n l√Ω h·ªçc sinh v√† ho·∫°t ƒë·ªông gi·∫£ng d·∫°y.

    NGUY√äN T·∫ÆC HO·∫†T ƒê·ªòNG:
    1. **Th·ª±c hi·ªán tr∆∞·ªõc, b√°o c√°o sau**: Lu√¥n ∆∞u ti√™n th·ª±c hi·ªán y√™u c·∫ßu c·ªßa gia s∆∞ tr∆∞·ªõc, sau ƒë√≥ b√°o c√°o k·∫øt qu·∫£ chi ti·∫øt.
    
    2. **Mapping th√¥ng minh ch·ªß ƒë·ªÅ**: Khi gia s∆∞ n√≥i ch·ªß ƒë·ªÅ chung, h√£y map sang ch·ªß ƒë·ªÅ c·ª• th·ªÉ:
       - "h√¨nh h·ªçc" ‚Üí d√πng "t·ª© gi√°c" (c√≥ s·∫µn trong h·ªá th·ªëng)
       - "ƒë·∫°i s·ªë" ‚Üí d√πng "ph∆∞∆°ng tr√¨nh" (c√≥ s·∫µn trong h·ªá th·ªëng)
       - Lu√¥n th·ª≠ map tr∆∞·ªõc khi b√°o l·ªói kh√¥ng t√¨m th·∫•y
    
    3. **Y√™u c·∫ßu th√¥ng tin thi·∫øu**: Ch·ªâ h·ªèi l·∫°i khi TH·ª∞C S·ª∞ thi·∫øu th√¥ng tin c·∫ßn thi·∫øt:
       - "Giao b√†i t·∫≠p cho An" (thi·∫øu ch·ªß ƒë·ªÅ v√† s·ªë c√¢u) ‚Üí H·ªèi l·∫°i
       - "Ch·∫•m b√†i" (thi·∫øu submission ID) ‚Üí Li·ªát k√™ b√†i n·ªôp available
       - "Th√™m ghi ch√∫ cho B√¨nh: [n·ªôi dung c·ª• th·ªÉ]" ‚Üí TH·ª∞C HI·ªÜN NGAY, kh√¥ng h·ªèi
    
    4. **X√°c nh·∫≠n h√†nh ƒë·ªông**: CH·ªà x√°c nh·∫≠n cho c√°c tr∆∞·ªùng h·ª£p TH·ª∞C S·ª∞ nguy hi·ªÉm:
       - Ch·∫•m ƒëi·ªÉm d∆∞·ªõi 30 (c√≥ th·ªÉ l√†m h·ªçc sinh bu·ªìn)
       - Giao qu√° 15 b√†i t·∫≠p c√πng l√∫c (qu√° t·∫£i)
       - X√≥a d·ªØ li·ªáu (kh√¥ng √°p d·ª•ng trong h·ªá th·ªëng n√†y)
       
       KH√îNG X√ÅC NH·∫¨N cho:
       - Th√™m ghi ch√∫ b√¨nh th∆∞·ªùng
       - Giao b√†i t·∫≠p s·ªë l∆∞·ª£ng h·ª£p l√Ω (<15)
       - T·∫°o l·ªô tr√¨nh h·ªçc t·∫≠p
       - Ch·∫•m ƒëi·ªÉm b√¨nh th∆∞·ªùng (30-100)
    
    5. **Giao ti·∫øp th√¢n thi·ªán**: 
       - X∆∞ng "em", g·ªçi gia s∆∞ "th·∫ßy/c√¥"
       - S·ª≠ d·ª•ng emoji: üìö‚úÖ‚ö†Ô∏èüéØ
       - B√°o c√°o k·∫øt qu·∫£ chi ti·∫øt sau khi th·ª±c hi·ªán
    
    6. **H·ªó tr·ª£ proactive**: 
       - G·ª£i √Ω h√†nh ƒë·ªông ti·∫øp theo
       - C·∫£nh b√°o n·∫øu c√≥ v·∫•n ƒë·ªÅ
       - Cung c·∫•p th·ªëng k√™ h·ªØu √≠ch

    C√ÅC C√îNG C·ª§ AVAILABLE:
    - assign_exercise: Giao b√†i t·∫≠p cho h·ªçc sinh
    - get_student_activity_log: Xem ho·∫°t ƒë·ªông c·ªßa h·ªçc sinh  
    - grade_submission: Ch·∫•m ƒëi·ªÉm b√†i n·ªôp
    - add_note_to_report: Th√™m ghi ch√∫ v√†o b√°o c√°o h·ªçc sinh
    - create_custom_pathway: T·∫°o l·ªô tr√¨nh h·ªçc t·∫≠p t√πy ch·ªânh
    - list_available_submissions: Li·ªát k√™ b√†i n·ªôp c√≥ th·ªÉ ch·∫•m

    QUAN TR·ªåNG: H√£y TH·ª∞C HI·ªÜN y√™u c·∫ßu tr∆∞·ªõc, sau ƒë√≥ b√°o c√°o k·∫øt qu·∫£. ƒê·ª´ng h·ªèi x√°c nh·∫≠n kh√¥ng c·∫ßn thi·∫øt!
    """
    
    model = genai.GenerativeModel(
        model_name="gemini-2.5-flash",
        tools=[tool],
        system_instruction=system_instruction
    )
    
    return model


def execute_function_call(function_name: str, args: Dict[str, Any]) -> str:
    """Execute the appropriate function based on function name and arguments."""
    if function_name == "assign_exercise":
        return assign_exercise(
            student_name=args.get("student_name"),
            learning_object_title=args.get("learning_object_title"),
            num_questions=args.get("num_questions")
        )
    elif function_name == "get_student_activity_log":
        return get_student_activity_log(
            student_name=args.get("student_name"),
            date_range=args.get("date_range", "today")
        )
    elif function_name == "grade_submission":
        return grade_submission(
            submission_id=args.get("submission_id"),
            score=args.get("score"),
            feedback_text=args.get("feedback_text")
        )
    elif function_name == "add_note_to_report":
        return add_note_to_report(
            student_name=args.get("student_name"),
            note_text=args.get("note_text")
        )
    elif function_name == "create_custom_pathway":
        return create_custom_pathway(
            student_name=args.get("student_name"),
            learning_object_titles=args.get("learning_object_titles", [])
        )
    elif function_name == "list_available_submissions":
        return list_available_submissions()
    else:
        return f"Unknown function: {function_name}"


def run_agent_flow(prompt: str) -> Dict[str, Any]:
    """
    Run the complete agent flow with function calling capability and detailed logging.
    
    Args:
        prompt: User's natural language command
        
    Returns:
        Dictionary containing final response and detailed logs
    """
    logs = []
    start_time = datetime.now()
    turn_count = 0  # Initialize turn_count early to avoid reference errors
    
    def add_log(step: str, status: str, message: str, details: Dict[str, Any] = None):
        """Add a log entry with timestamp."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "status": status,  # "processing", "success", "error", "info"
            "message": message,
            "details": details or {}
        }
        logs.append(log_entry)
        return log_entry
    
    try:
        add_log("initialization", "processing", "ü§ñ Kh·ªüi t·∫°o AI Agent v·ªõi Gemini API...")
        
        model = configure_gemini()
        chat = model.start_chat()
        
        add_log("initialization", "success", "‚úÖ Kh·ªüi t·∫°o th√†nh c√¥ng Gemini model")
        add_log("prompt_analysis", "processing", f"üìù Ph√¢n t√≠ch l·ªánh: '{prompt}'")
        
        # Send initial prompt
        response = chat.send_message(prompt)
        add_log("prompt_analysis", "success", "‚úÖ ƒê√£ g·ª≠i l·ªánh ƒë·∫øn AI, ƒëang ch·ªù ph·∫£n h·ªìi...")
        
        max_turns = 10  # Prevent infinite loops
        
        # Handle function calling loop
        while response.candidates[0].content.parts and turn_count < max_turns:
            turn_count += 1
            add_log("processing", "info", f"üîÑ X·ª≠ l√Ω turn {turn_count}")
            
            # Check if we have any function calls in this turn
            function_calls = []
            text_parts = []
            
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'function_call') and part.function_call:
                    function_calls.append(part.function_call)
                elif hasattr(part, 'text') and part.text:
                    text_parts.append(part.text)
            
            # If we have function calls, execute them all
            if function_calls:
                add_log("function_detection", "success", f"üõ†Ô∏è Ph√°t hi·ªán {len(function_calls)} function call(s)")
                
                function_responses = []
                
                for i, function_call in enumerate(function_calls):
                    function_name = function_call.name
                    function_args = {}
                    
                    # Extract function arguments - convert protobuf to native Python types
                    for key, value in function_call.args.items():
                        # Convert protobuf RepeatedComposite to native Python list
                        if hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                            try:
                                # Handle list/array types from protobuf
                                function_args[key] = list(value)
                            except:
                                # Fallback to original value if conversion fails
                                function_args[key] = value
                        else:
                            # Handle scalar types
                            function_args[key] = value
                    
                    add_log("function_execution", "processing", 
                           f"‚öôÔ∏è Th·ª±c thi function {i+1}/{len(function_calls)}: {function_name}",
                           {"function_name": function_name, "arguments": function_args})
                    
                    # Execute the function
                    function_result = execute_function_call(function_name, function_args)
                    
                    add_log("function_execution", "success", 
                           f"‚úÖ Ho√†n th√†nh {function_name}",
                           {"result_preview": function_result[:100] + "..." if len(function_result) > 100 else function_result})
                    
                    # Add to response list
                    function_responses.append({
                        "function_response": {
                            "name": function_name,
                            "response": {"result": function_result}
                        }
                    })
                
                add_log("function_results", "processing", "üì§ G·ª≠i k·∫øt qu·∫£ functions tr·ªü l·∫°i AI...")
                
                # Send all function results back to model
                response = chat.send_message(function_responses)
                
                add_log("function_results", "success", "‚úÖ AI ƒë√£ nh·∫≠n k·∫øt qu·∫£ v√† ƒëang t·ªïng h·ª£p ph·∫£n h·ªìi...")
            
            # If we have text response and no function calls, return it
            elif text_parts:
                final_response = " ".join(text_parts)
                add_log("completion", "success", "üéâ Ho√†n th√†nh! T·∫°o ph·∫£n h·ªìi cu·ªëi c√πng cho ng∆∞·ªùi d√πng")
                
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                
                return {
                    "response": final_response,
                    "logs": logs,
                    "processing_time": processing_time,
                    "turns_processed": turn_count,
                    "status": "success"
                }
            
            else:
                add_log("processing", "info", "‚è∏Ô∏è Kh√¥ng c√≥ function call ho·∫∑c text response, tho√°t v√≤ng l·∫∑p")
                break
        
        # If we get here, try to extract any available text response
        if response.candidates[0].content.parts:
            final_parts = []
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    final_parts.append(part.text)
            
            if final_parts:
                final_response = " ".join(final_parts)
                add_log("completion", "success", "‚úÖ Tr√≠ch xu·∫•t ƒë∆∞·ª£c ph·∫£n h·ªìi cu·ªëi c√πng")
                
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                
                return {
                    "response": final_response,
                    "logs": logs,
                    "processing_time": processing_time,
                    "turns_processed": turn_count,
                    "status": "success"
                }
        
        add_log("completion", "error", "‚ùå Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi cu·ªëi c√πng")
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        return {
            "response": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n.",
            "logs": logs,
            "processing_time": processing_time,
            "turns_processed": turn_count,
            "status": "error"
        }
        
    except Exception as e:
        add_log("error", "error", f"‚ùå L·ªói h·ªá th·ªëng: {str(e)}")
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        return {
            "response": f"ƒê√£ x·∫£y ra l·ªói: {str(e)}",
            "logs": logs,
            "processing_time": processing_time,
            "turns_processed": turn_count,
            "status": "error"
        }
        
        # If we get here, return the last text response if available
        if response.candidates[0].content.parts:
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    return part.text
        
        return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n."
        
    except Exception as e:
        return f"ƒê√£ x·∫£y ra l·ªói: {str(e)}"