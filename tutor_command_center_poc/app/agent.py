import os
import json
from datetime import datetime
from typing import Dict, Any, List
from dotenv import load_dotenv
import google.generativeai as genai
from google.generativeai.types import FunctionDeclaration, Tool

from .tools import assign_exercise, get_student_activity_log, grade_submission, add_note_to_report, create_custom_pathway

# Load environment variables
load_dotenv()


def configure_gemini():
    """Configure Gemini API with tools."""
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    
    # Define function declarations for Gemini
    assign_exercise_func = FunctionDeclaration(
        name="assign_exercise",
        description="Assign exercises to a student based on learning object title",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student to assign exercises to"
                },
                "learning_object_title": {
                    "type": "string",
                    "description": "The title of the learning object/topic"
                },
                "num_questions": {
                    "type": "integer",
                    "description": "Number of questions to assign"
                }
            },
            "required": ["student_name", "learning_object_title", "num_questions"]
        }
    )
    
    get_activity_log_func = FunctionDeclaration(
        name="get_student_activity_log",
        description="Get activity log for a specific student",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student"
                },
                "date_range": {
                    "type": "string",
                    "description": "Date range filter (e.g., 'today', 'this_week', etc.)"
                }
            },
            "required": ["student_name"]
        }
    )
    
    grade_submission_func = FunctionDeclaration(
        name="grade_submission",
        description="Grade a student submission and provide feedback",
        parameters={
            "type": "object",
            "properties": {
                "submission_id": {
                    "type": "string",
                    "description": "The ID of the submission to grade"
                },
                "score": {
                    "type": "number",
                    "description": "The score to assign (0-100)"
                },
                "feedback_text": {
                    "type": "string",
                    "description": "Detailed feedback text for the student"
                }
            },
            "required": ["submission_id", "score", "feedback_text"]
        }
    )
    
    add_note_func = FunctionDeclaration(
        name="add_note_to_report",
        description="Add a tutor note to student's progress report",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student"
                },
                "note_text": {
                    "type": "string",
                    "description": "The note text to add to the student's report"
                }
            },
            "required": ["student_name", "note_text"]
        }
    )
    
    create_pathway_func = FunctionDeclaration(
        name="create_custom_pathway",
        description="Create a custom learning pathway for a student with specific learning objects",
        parameters={
            "type": "object",
            "properties": {
                "student_name": {
                    "type": "string",
                    "description": "The name of the student"
                },
                "learning_object_titles": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "List of learning object titles to include in the pathway"
                }
            },
            "required": ["student_name", "learning_object_titles"]
        }
    )
    
    # Create tool with function declarations
    tool = Tool(function_declarations=[
        assign_exercise_func, 
        get_activity_log_func, 
        grade_submission_func, 
        add_note_func, 
        create_pathway_func
    ])
    
    # Initialize model with tools
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        tools=[tool]
    )
    
    return model


def execute_function_call(function_name: str, args: Dict[str, Any]) -> str:
    """Execute the appropriate function based on function name and arguments."""
    if function_name == "assign_exercise":
        return assign_exercise(
            student_name=args.get("student_name"),
            learning_object_title=args.get("learning_object_title"),
            num_questions=args.get("num_questions")
        )
    elif function_name == "get_student_activity_log":
        return get_student_activity_log(
            student_name=args.get("student_name"),
            date_range=args.get("date_range", "today")
        )
    elif function_name == "grade_submission":
        return grade_submission(
            submission_id=args.get("submission_id"),
            score=args.get("score"),
            feedback_text=args.get("feedback_text")
        )
    elif function_name == "add_note_to_report":
        return add_note_to_report(
            student_name=args.get("student_name"),
            note_text=args.get("note_text")
        )
    elif function_name == "create_custom_pathway":
        return create_custom_pathway(
            student_name=args.get("student_name"),
            learning_object_titles=args.get("learning_object_titles", [])
        )
    else:
        return f"Unknown function: {function_name}"


def run_agent_flow(prompt: str) -> Dict[str, Any]:
    """
    Run the complete agent flow with function calling capability and detailed logging.
    
    Args:
        prompt: User's natural language command
        
    Returns:
        Dictionary containing final response and detailed logs
    """
    logs = []
    start_time = datetime.now()
    
    def add_log(step: str, status: str, message: str, details: Dict[str, Any] = None):
        """Add a log entry with timestamp."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "status": status,  # "processing", "success", "error", "info"
            "message": message,
            "details": details or {}
        }
        logs.append(log_entry)
        return log_entry
    
    try:
        add_log("initialization", "processing", "ü§ñ Kh·ªüi t·∫°o AI Agent v·ªõi Gemini API...")
        
        model = configure_gemini()
        chat = model.start_chat()
        
        add_log("initialization", "success", "‚úÖ Kh·ªüi t·∫°o th√†nh c√¥ng Gemini model")
        add_log("prompt_analysis", "processing", f"üìù Ph√¢n t√≠ch l·ªánh: '{prompt}'")
        
        # Send initial prompt
        response = chat.send_message(prompt)
        add_log("prompt_analysis", "success", "‚úÖ ƒê√£ g·ª≠i l·ªánh ƒë·∫øn AI, ƒëang ch·ªù ph·∫£n h·ªìi...")
        
        turn_count = 0
        max_turns = 10  # Prevent infinite loops
        
        # Handle function calling loop
        while response.candidates[0].content.parts and turn_count < max_turns:
            turn_count += 1
            add_log("processing", "info", f"üîÑ X·ª≠ l√Ω turn {turn_count}")
            
            # Check if we have any function calls in this turn
            function_calls = []
            text_parts = []
            
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'function_call') and part.function_call:
                    function_calls.append(part.function_call)
                elif hasattr(part, 'text') and part.text:
                    text_parts.append(part.text)
            
            # If we have function calls, execute them all
            if function_calls:
                add_log("function_detection", "success", f"üõ†Ô∏è Ph√°t hi·ªán {len(function_calls)} function call(s)")
                
                function_responses = []
                
                for i, function_call in enumerate(function_calls):
                    function_name = function_call.name
                    function_args = {}
                    
                    # Extract function arguments - convert protobuf to native Python types
                    for key, value in function_call.args.items():
                        # Convert protobuf RepeatedComposite to native Python list
                        if hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                            try:
                                # Handle list/array types from protobuf
                                function_args[key] = list(value)
                            except:
                                # Fallback to original value if conversion fails
                                function_args[key] = value
                        else:
                            # Handle scalar types
                            function_args[key] = value
                    
                    add_log("function_execution", "processing", 
                           f"‚öôÔ∏è Th·ª±c thi function {i+1}/{len(function_calls)}: {function_name}",
                           {"function_name": function_name, "arguments": function_args})
                    
                    # Execute the function
                    function_result = execute_function_call(function_name, function_args)
                    
                    add_log("function_execution", "success", 
                           f"‚úÖ Ho√†n th√†nh {function_name}",
                           {"result_preview": function_result[:100] + "..." if len(function_result) > 100 else function_result})
                    
                    # Add to response list
                    function_responses.append({
                        "function_response": {
                            "name": function_name,
                            "response": {"result": function_result}
                        }
                    })
                
                add_log("function_results", "processing", "üì§ G·ª≠i k·∫øt qu·∫£ functions tr·ªü l·∫°i AI...")
                
                # Send all function results back to model
                response = chat.send_message(function_responses)
                
                add_log("function_results", "success", "‚úÖ AI ƒë√£ nh·∫≠n k·∫øt qu·∫£ v√† ƒëang t·ªïng h·ª£p ph·∫£n h·ªìi...")
            
            # If we have text response and no function calls, return it
            elif text_parts:
                final_response = " ".join(text_parts)
                add_log("completion", "success", "üéâ Ho√†n th√†nh! T·∫°o ph·∫£n h·ªìi cu·ªëi c√πng cho ng∆∞·ªùi d√πng")
                
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                
                return {
                    "response": final_response,
                    "logs": logs,
                    "processing_time": processing_time,
                    "turns_processed": turn_count,
                    "status": "success"
                }
            
            else:
                add_log("processing", "info", "‚è∏Ô∏è Kh√¥ng c√≥ function call ho·∫∑c text response, tho√°t v√≤ng l·∫∑p")
                break
        
        # If we get here, try to extract any available text response
        if response.candidates[0].content.parts:
            final_parts = []
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    final_parts.append(part.text)
            
            if final_parts:
                final_response = " ".join(final_parts)
                add_log("completion", "success", "‚úÖ Tr√≠ch xu·∫•t ƒë∆∞·ª£c ph·∫£n h·ªìi cu·ªëi c√πng")
                
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                
                return {
                    "response": final_response,
                    "logs": logs,
                    "processing_time": processing_time,
                    "turns_processed": turn_count,
                    "status": "success"
                }
        
        add_log("completion", "error", "‚ùå Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi cu·ªëi c√πng")
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        return {
            "response": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n.",
            "logs": logs,
            "processing_time": processing_time,
            "turns_processed": turn_count,
            "status": "error"
        }
        
    except Exception as e:
        add_log("error", "error", f"‚ùå L·ªói h·ªá th·ªëng: {str(e)}")
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        return {
            "response": f"ƒê√£ x·∫£y ra l·ªói: {str(e)}",
            "logs": logs,
            "processing_time": processing_time,
            "turns_processed": turn_count,
            "status": "error"
        }
        
        # If we get here, return the last text response if available
        if response.candidates[0].content.parts:
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    return part.text
        
        return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu c·ªßa b·∫°n."
        
    except Exception as e:
        return f"ƒê√£ x·∫£y ra l·ªói: {str(e)}"